{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAI612: Deep Learning & its Applications\n",
    "\n",
    "*Notebook 7.3: Loading Pretrained Embeddings*\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/harmanani/AAI612/blob/main/Week7/Notebook7.3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe MIT License (MIT)\\nCopyright (c) 2021 NVIDIA\\nPermission is hereby granted, free of charge, to any person obtaining a copy of\\nthis software and associated documentation files (the \"Software\"), to deal in\\nthe Software without restriction, including without limitation the rights to\\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\\nthe Software, and to permit persons to whom the Software is furnished to do so,\\nsubject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "thex Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and unzip the precomputed embeddings from 2014 English Wikipedia `glove.6B.zip` from Go to https://nlp.stanford.edu/projects/glove. It’s an 822 MB zip file called glove.6B.zip, containing 100-dimensional embedding vectors for 400,000 words (or nonword tokens). Unzip it inside a directory `data`.  Keep the file `glove.6B.100d.txt` an delete the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproessing the Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Read the embeddings.  Start by opening the file and read it line by line. Split each line into its elements. Extract the first element, which represents the word itself, and then create a vector from the remaining elements and insert the word and the corresponding vector into a dictionary, which serves as the return value of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial\n",
    "\n",
    "# Read embeddings from file.\n",
    "def read_embeddings():\n",
    "    FILE_NAME = 'data/glove.6B.100d.txt'\n",
    "    embeddings = {}\n",
    "    file = open(FILE_NAME, 'r', encoding='utf-8')\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector\n",
    "    file.close()\n",
    "    print('Read %s embeddings.' % len(embeddings))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cosine distance between a specific embedding and all other embeddings. It then prints the n closest ones.  Euclidean distance would also have worked fine, but the results would sometimes be different because the GloVe vectors are not normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "def print_n_closest(embeddings, vec0, n):\n",
    "    word_distances = {}\n",
    "    for (word, vec1) in embeddings.items():\n",
    "        distance = scipy.spatial.distance.cosine(vec1, vec0)\n",
    "        word_distances[distance] = word\n",
    "    # Print words sorted by distance.\n",
    "    for distance in sorted(word_distances.keys())[:n]:\n",
    "        word = word_distances[distance]\n",
    "        print(word + ': %6.3f' % distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "First read the embeddings by invoking `read_embeddings()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 400000 embeddings.\n"
     ]
    }
   ],
   "source": [
    "embeddings = read_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the embeddings for **hello** and print closest emebdding using `print_n_closest()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words closest to hello\n",
      "hello:  0.000\n",
      "goodbye:  0.209\n",
      "hey:  0.283\n"
     ]
    }
   ],
   "source": [
    "lookup_word = 'hello'\n",
    "print('\\nWords closest to ' + lookup_word)\n",
    "print_n_closest(embeddings, embeddings[lookup_word], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Retrieve the embeddings for **dog** and print closest emebdding using `print_n_closest()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words closest to dog\n",
      "dog:  0.000\n",
      "cat:  0.120\n",
      "dogs:  0.166\n"
     ]
    }
   ],
   "source": [
    "lookup_word = 'dog'\n",
    "print('\\nWords closest to ' + lookup_word)\n",
    "print_n_closest(embeddings, embeddings[lookup_word], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the capital of Jordan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amman:  0.250\n",
      "jordan:  0.268\n",
      "cairo:  0.321\n"
     ]
    }
   ],
   "source": [
    "vec = embeddings['beirut'] - embeddings['lebanon'] + embeddings['jordan']\n",
    "print_n_closest(embeddings, vec, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### King - man + Woman = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the embeddings for **king** and print closest emebdding using `print_n_closest()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words closest to king\n",
      "king:  0.000\n",
      "prince:  0.232\n",
      "queen:  0.249\n"
     ]
    }
   ],
   "source": [
    "lookup_word = 'king'\n",
    "print('\\nWords closest to ' + lookup_word)\n",
    "print_n_closest(embeddings, embeddings[lookup_word], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the words closest to the vector resulting from computing `(king - man + woman).`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words closest to (king - man + woman)\n",
      "king:  0.145\n",
      "queen:  0.217\n",
      "monarch:  0.307\n"
     ]
    }
   ],
   "source": [
    "lookup_word = '(king - man + woman)'\n",
    "print('\\nWords closest to ' + lookup_word)\n",
    "vec = embeddings['king'] - embeddings['man'] + embeddings['woman']\n",
    "print_n_closest(embeddings, vec, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Madrid − Spain + Sweden = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "A more impressive example next where we first print the words closest to **sweden** and **madrid** and then print the words closest to the result from the computation `(madrid − spain + sweden).`  We would assume the answer to be `Stockholm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words closest to sweden\n",
      "sweden:  0.000\n",
      "denmark:  0.138\n",
      "norway:  0.193\n"
     ]
    }
   ],
   "source": [
    "lookup_word = 'sweden'\n",
    "print('\\nWords closest to ' + lookup_word)\n",
    "print_n_closest(embeddings, embeddings[lookup_word], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words closest to madrid\n",
      "madrid:  0.000\n",
      "barcelona:  0.157\n",
      "valencia:  0.197\n"
     ]
    }
   ],
   "source": [
    "lookup_word = 'madrid'\n",
    "print('\\nWords closest to ' + lookup_word)\n",
    "print_n_closest(embeddings, embeddings[lookup_word], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, print the words closest to the result from the computation `(madrid − spain + sweden).`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words closest to (madrid - spain + sweden)\n",
      "stockholm:  0.271\n",
      "sweden:  0.300\n",
      "copenhagen:  0.305\n"
     ]
    }
   ],
   "source": [
    "lookup_word = '(madrid - spain + sweden)'\n",
    "print('\\nWords closest to ' + lookup_word)\n",
    "vec = embeddings['madrid'] - embeddings['spain'] + embeddings['sweden']\n",
    "print_n_closest(embeddings, vec, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
