{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# AAI612: Deep Learning & its Applications\n",
    "\n",
    "\n",
    "*Notebook 3.1: Activation Functions*\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/harmanani/AAI612/blob/main/Week3/Notebook3.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions are biologically inspired and are abstractions representing the rate of action potential firing in the cell.  They define the output of a node given a set of inputs while keeping this output restricted to a certain limit.  The importance of activation functions stems from the fact that they introduce non-linearity into the neural network.  They should be nonlinear to encode complex patterns of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desirable Features of an Activation Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ability to avoid the vanishing gradient problem.\n",
    "\n",
    "2. Zero-Centered: Output of the activation function should be symmetrical at zero so that the gradients do not shift to a particular direction.\n",
    "\n",
    "3. Computational Expense: Activation functions are applied after every layer and need to be calculated millions of times in deep networks. Hence, they should be computationally inexpensive to calculate.\n",
    "\n",
    "4. Differentiable: As mentioned, neural networks are trained using the gradient descent process, hence the layers in the model need to differentiable or at least differentiable in parts. This is a necessary requirement for a function to work as activation function layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear activation function takes the inputs, multiplied by the weights for each neuron, and creates an output signal proportional to the input. In one sense, a linear function is better than a step function because it allows multiple outputs, not just yes and no.\n",
    "However, a linear activation function has two major problems:\n",
    "1. Not possible to use backpropagation (gradient descent) to train the model — the derivative of the function is a constant, and has no relation to the input, X. So it’s not possible to go back and understand which weights in the input neurons can provide a better prediction.\n",
    "2. All layers of the neural network collapse into one — with linear activation functions, no matter how many layers in the neural network, the last layer will be a linear function of the first layer (because a linear combination of linear functions is still a linear function). So a linear activation function turns the neural network into just one layer.\n",
    "A neural network with a linear activation function is simply a linear regression model. It has limited power and ability to handle complexity varying parameters of input data.\n",
    "And that’s why linear activation function is hardly used in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(-8, 8, 1000)\n",
    "y_values = x_values\n",
    "plt.plot(x_values, y_values, 'red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function is a special form of the logistic function and is usually denoted by $\\sigma(x)$. It is given by:\n",
    "\n",
    "$\\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "The sigmoid function is called a squashing function as its domain is the set of all real numbers, and its range is (0, 1). The shape of the function for all possible inputs is an S-shape from zero up through 0.5 to 1.0.  If the input to the function is either a very large negative number or a very large positive number,  the output is always between 0 and 1.  Same goes for any number between $-\\infty$ and $+\\infty$.  \n",
    "\n",
    "Using a linear activation function in a neural network would lead to a model that can only learn linearly separable problems.  We can add hidden layers and sigmoid activation functions in order for the neural network to learn a non-linearly separable problem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(-8, 8, 1000)\n",
    "f = (1/(1+np.exp(-x_values)))\n",
    "plt.plot(x_values, f, 'red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"Sigmoid(X)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperbolic Tangent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperbolic tangent function, or tanh for short, is a nonlinear activation function that is similar to the sigmoid function (in terms of shaep).  Its output values vary between -1.0 and 1.0. In the later 1990s and through the 2000s, the tanh function was preferred over the sigmoid activation function as models that used it were easier to train and often had better predictive performance.  It is generally noted that the hyperbolic tangent activation function typically performs better than the logistic sigmoid.\n",
    "\n",
    "\n",
    "\n",
    "$\\text{Tanh}(x) = \\tanh(x) = \\frac{e^{x} - e^{-x}} {e^x + e^{-x}}$\n",
    "\n",
    "Two main issues with both the sigmoid and tanh functions:\n",
    "\n",
    "1. A general problem is that both the sigmoid and tanh functions saturate. This means that large values snap to 1.0 and small values snap to -1 or 0 for tanh and sigmoid respectively. Further, the functions are only really sensitive to changes around their mid-point of their input, such as 0.5 for sigmoid and 0.0 for tanh.\n",
    "\n",
    "2. It was noted that layers deep in large networks using these nonlinear activation functions fail to receive useful gradient information. Error is back propagated through the network and used to update the weights. The amount of error decreases dramatically with each additional layer through which it is propagated, given the derivative of the chosen activation function. This is called the vanishing gradient problem and prevents deep (multi-layered) networks from learning effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(-3, 3, 10000)\n",
    "f = np.tanh(x_values)\n",
    "plt.plot(x_values, f, 'red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"Sigmoid(X)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have shown earlier in module I, logistic regression produces a decimal between 0 and 1.0. For example, a logistic regression output of 0.8 from an email classifier suggests an 80% chance of an email being spam and a 20% chance of it being not spam. Clearly, the sum of the probabilities of an email being either spam or not spam is 1.0.  Softmax extends this idea into a multi-class world. That is, Softmax assigns decimal probabilities to each class in a multi-class problem. Those decimal probabilities must add up to 1.0. This additional constraint helps training converge more quickly than it otherwise would.  The softmax activation function generates a vector of (normalized) probabilities with one value for each possible class.\n",
    "\n",
    "The Softmax activation function is given as: $\\text{Softmax}(x_{z}) = \\frac{e^{x_z}}{\\sum_z e^{x_z}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ''' Compute softmax values for each sets of scores in x. '''\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(-3, 3, 10000)\n",
    "plt.plot(x_values, softmax(x_values))\n",
    "plt.axis('tight')\n",
    "plt.ylabel(\"Softmax(X)\")\n",
    "plt.xlabel('x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectified Linear Function (ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use stochastic gradient descent with backpropagation of errors to train deep neural networks, an activation function is needed that looks and acts like a linear function, but is, in fact, a nonlinear function allowing complex relationships in the data to be learned.  The function must also provide more sensitivity to the activation sum input and avoid easy saturation.  The solution is to use the rectified linear activation function.  A node or unit that implements this activation function is referred to as a rectified linear activation unit, or ReLU for short. Often, networks that use the rectifier function for the hidden layers are referred to as rectified networks.\n",
    "\n",
    "The ReLU function, $\\text{ReLU}(x) = (x)^+ = \\max(0, x)$, returns 0 if the x (input) is less than 0 and x otherwise.\n",
    "if the x (input) is greater than 0\n",
    "\n",
    "Because rectified linear units are nearly linear, they preserve many of the properties that make linear models easy to optimize with gradient-based methods. They also preserve many of the properties that make linear models generalize well.  The advantages of ReLU are:\n",
    "\n",
    "1. Computational Simplicity.\n",
    "2. Representational Sparsity\n",
    "3. Linear Behavior\n",
    "4. Train Deep Networks\n",
    "\n",
    "It should be noted that when the value in the ReLu function is negative, no learning happens as the new weight remains equal to the old weight since the value of the derivative is 0. This is called the “Dead Neuron” issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    ''' It returns zero if the input is less than zero otherwise it returns the given input. '''\n",
    "    x1=[]\n",
    "    for i in x:\n",
    "        if i<0:\n",
    "            x1.append(0)\n",
    "        else:\n",
    "            x1.append(i)\n",
    "\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(-10, 10, 10000)\n",
    "plt.plot(x_values, ReLU(x_values))\n",
    "plt.axis('tight')\n",
    "plt.ylabel(\"ReLU(X)\")\n",
    "plt.xlabel('x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky Rectified Linear Function (LReLU)\n",
    "\n",
    "The Leaky ReLU activation function was developed to overcome one of the major shortcomings of ReLU activation function. Recall, the ReLU activation function gives the derivate as 1 when the value is positive. In case when the value is negative, the derivative becomes 0 during backpropagation.  This means that when the value is negative, no learning happens since during the backpropagation stage, zero values' gradient descents become zero again and they do not converge to good local minimum. It is a dead end situation. Leaky ReLU substitutes zero values with some small value say 0.001 (referred as “alpha”). So, for leaky ReLU, the function is:\n",
    "\n",
    "$f(x) = max(0.001x, x)$\n",
    "\n",
    "Now gradient descent of 0.001x will be having a non-zero value and it will continue learning without reaching dead end. Hence, leaky ReLU performs better than ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LReLU(x):\n",
    "    ''' It returns zero if the input is less than zero otherwise it returns the given input. '''\n",
    "    return np.where(x > 0, x, x * 0.01) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(-100, 100, 100000)\n",
    "plt.plot(x_values, LReLU(x_values))\n",
    "plt.axis('tight')\n",
    "plt.ylabel(\"LReLU(X)\")\n",
    "plt.xlabel('x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swish is a smooth, non-monotonic function that consistently matches or outperforms ReLU on deep networks applied to a variety of challenging domains such as Image classification and Machine translation. \n",
    "\n",
    "Swish is an activation function, $ f(x) = x . \\sigma(\\beta x)$ where $\\beta$ is a learnable parameter. Nearly all implementations do not use the learnable parameter $\\beta$ in which case the activation function is $x.\\sigma(x)$ (\"Swish-1\").\n",
    "\n",
    "The function $x.\\sigma(x)$ is exactly the SiLU, which was introduced by other authors before the swish. See Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear Unit) was originally coined, and see Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning and Swish: a Self-Gated Activation Function where the same activation function was experimented with later.\n",
    "\n",
    "With MNIST data set, when Swish and ReLU are compared, both activation functions achieve similar performances up to 40 layers. However, Swish outperforms ReLU by a large margin in the range between 40 and 50 layers when optimization becomes difficult. In very deep networks, Swish achieves higher test accuracy than ReLU. In terms of batch size, the performance of both activation functions decrease as batch size increases, potentially due to sharp minima (Keskar et al., 2017). However, Swish outperforms ReLU on every batch size, suggesting that the performance difference between the two activation functions remains even when varying the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(-8, 8, 10000)\n",
    "f = (x_values/(1+np.exp(-x_values)))\n",
    "plt.plot(x_values, f, 'red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"Swish(X)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tanhx+0.2x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can complicate the The hyperbolic tangent function her by adding a linear term 0.2*x to the output to prevent the derivative from approaching 0:\n",
    "\n",
    "\n",
    "\n",
    "$f(x) = \\frac{e^{x} - e^{-x}} {e^x + e^{-x}}+0.2*x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(-3, 3, 10000)\n",
    "f = np.tanh(x_values) + 0.2*x_values\n",
    "plt.plot(x_values, f, 'red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"Tanh(X) + 0.2*x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Exponential Linear Unit (ELU) are activation functions which, in contrast to ReLUs, have negative values which allow them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information.\n",
    "\n",
    "Elu is implemented as follows:\n",
    "\n",
    "The exponential linear unit (ELU) with $\\alpha > 0$ is:\n",
    "\n",
    "$f(x) = x$ if $x>0$\n",
    "\n",
    "$\\alpha(e^x -1)$ if $x \\leq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(x, alpha):\n",
    "    return np.where(x > 0, x, alpha*(np.e**x-1)) \n",
    "\n",
    "x_values = np.linspace(-3, 3, 10000)\n",
    "f = elu(x_values, 1.0)\n",
    "plt.plot(x_values, f, 'red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"softplus\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## softplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softplus activation function uses the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = ln(1+e^x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softplus function can be viewed as a smooth version of ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(-3, 3, 10000)\n",
    "f = np.log(1+np.e**x_values)\n",
    "plt.plot(x_values, f, 'red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"softplus\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C1_W1_Lab_1_hello_world_nn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
